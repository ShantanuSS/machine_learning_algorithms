{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To find distance b/w two nodes, one can use Euclidean distance (other methods can also be used instead of this)\n",
    "def get_euclidean_distance(p1, p2):\n",
    "    # Convert to numpy arrays\n",
    "    p1 = np.array(p1)\n",
    "    p2 = np.array(p2)\n",
    "\n",
    "    distance = 0\n",
    "    # Iterate over each dimension of vector\n",
    "    for i in range(len(p1)-1):\n",
    "        distance += (p2[i] - p1[i]) ** 2\n",
    "    # Take the square root\n",
    "    return np.sqrt(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(k, training_set, testing_value):\n",
    "    distances = []\n",
    "    for i in range(len(training_set)):\n",
    "        # find the distance\n",
    "        # -1 to ignore target variable\n",
    "        dist = get_euclidean_distance(training_set[i][:-1], testing_value)\n",
    "        distances.append((training_set[i], dist))\n",
    "    # Sort list of tuples by distance\n",
    "    distances.sort(key=lambda x: x[1])\n",
    "\n",
    "    # List to store K nearest neighbors\n",
    "    neighbors = []\n",
    "    for i in range(k):\n",
    "        neighbors.append(distances[i][0])\n",
    "\n",
    "    # Determine the class of test instance\n",
    "    classes = {}\n",
    "    for i in range(len(neighbors)):\n",
    "        # [-1] will give the target\n",
    "        response  = neighbors[i][-1]\n",
    "        if response in classes:\n",
    "            classes[response] += 1\n",
    "        else:\n",
    "            classes[response] = 1\n",
    "    \n",
    "    # Sort the classes by frequency in descending order\n",
    "    sorted_classes = sorted(classes.items(), key=lambda x: x[1], reverse=True)\n",
    "    return sorted_classes[0][0]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(dataset, test_size=0.25):\n",
    "    n_test = int(len(dataset) * test_size)\n",
    "    # Getting sample test\n",
    "    test_set = dataset.sample(n_test, random_state=2)\n",
    "    train_set = []\n",
    "    for idx in dataset.index:\n",
    "        if idx in test_set.index:\n",
    "            continue\n",
    "        train_set.append(dataset.iloc[idx])\n",
    "    train_set = pd.DataFrame(train_set).astype(float).values.tolist()\n",
    "    test_set = test_set.astype(float).values.tolist()\n",
    "    \n",
    "    return train_set, test_set\n",
    "            \n",
    "    \n",
    "def get_accuracy(y_true, y_pred):\n",
    "    n_correct = 0\n",
    "    # Iterate over both arrays at same time\n",
    "    for act, pred in zip(y_true, y_pred):\n",
    "        # If they are same at same position, increment 1\n",
    "        if act == pred:\n",
    "            n_correct += 1\n",
    "    # Accuracy is total_correct / total\n",
    "    accuracy = n_correct /len(y_true)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(113, 37)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can use same dataset i.e Iris Dataset\n",
    "from sklearn.datasets import load_iris\n",
    "iris_dataset = load_iris()\n",
    "# Get target variables\n",
    "y_iris = iris_dataset.target\n",
    "# Create dataframe by using columns and data mentioned in iris_dataset\n",
    "iris_dataset = pd.DataFrame(iris_dataset.data, columns =iris_dataset.feature_names)\n",
    "# Add target variables to the dataframe \n",
    "iris_dataset = pd.concat([iris_dataset, pd.Series(y_iris)], axis=1)\n",
    "# target variable in above dataframe has header '0', rename it to something else\n",
    "iris_dataset.rename(columns={0:'class'}, inplace=True)\n",
    "\n",
    "train_set, test_set = train_test_split(iris_dataset)\n",
    "len(train_set), len(test_set)\n",
    "# iris_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get actual YLabel of Test set\n",
    "actual_class = np.array(test_set)[:, -1] # :, -1 will give last element of each row\n",
    "\n",
    "def get_test_data_accuracy(k):\n",
    "    preds = []\n",
    "    for row in test_set:\n",
    "        # Get X variable(leaving y_label)\n",
    "        predictors_only = row[:-1]\n",
    "\n",
    "        prediction = predict(k, train_set, predictors_only)\n",
    "        preds.append(prediction)\n",
    "\n",
    "    # Now we need to evaluate accuracy, which we can get by comparing predictions with actual result\n",
    "    curr_accuracy = get_accuracy(actual_class, preds)\n",
    "    return curr_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 0.9459459459459459),\n",
       " (3, 0.9459459459459459),\n",
       " (5, 0.918918918918919),\n",
       " (7, 0.918918918918919),\n",
       " (9, 0.918918918918919),\n",
       " (11, 0.918918918918919),\n",
       " (13, 0.918918918918919),\n",
       " (15, 0.9459459459459459),\n",
       " (17, 0.9459459459459459),\n",
       " (19, 0.9459459459459459),\n",
       " (21, 0.9459459459459459)]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We don't know what value of K we should be considering \n",
    "# We have to verify it manually. All it boils down to is surrounding the code from above in a loop.\n",
    "k_evaluations = []\n",
    "# Looping and skipping one element everytime, as we want our K to be odd to get a mode value afterwards.\n",
    "for k in range(1, 22, 2):\n",
    "    curr_accuracy = get_test_data_accuracy(k)\n",
    "    k_evaluations.append((k, curr_accuracy))\n",
    "\n",
    "k_evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction class:  2.0\n",
      "Prediction class name:  virginica\n"
     ]
    }
   ],
   "source": [
    "# So for this random state we can consider 3 as value of k\n",
    "# Now we can also make prediction for some unseen data\n",
    "random_single_test = [5,15,7,11]\n",
    "k_value = 3 \n",
    "y_predict = predict(k_value, train_set, random_single_test)\n",
    "print(\"Prediction class: \", y_predict)\n",
    "\n",
    "# Now from iris dataset set we know\n",
    "classes = {0:'setosa',1:'versicolor',2:'virginica'}\n",
    "# Therefore using this dict we can get the class name\n",
    "print(\"Prediction class name: \", classes[y_predict])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mldl] *",
   "language": "python",
   "name": "conda-env-mldl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
